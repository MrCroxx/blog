---
title: "《MapReduce: Simplified Data Processing on Large Clusters》论文翻译（MapReduce-OSDI04）"
date: 2020-08-08T12:21:45+08:00
lastmod: 2020-08-08T12:21:45+08:00
draft: false
keywords: []
description: ""
tags: ["MapReduce", "Translation"]
categories: ["Paper Reading"]
author: ""
---

*本篇文章是对论文[MapReduce-OSDI04](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/mapreduce-osdi04.pdf)的原创翻译，转载请严格遵守[CC BY-NC-SA协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)。*


<!--more-->

## 摘要

MapReduce是一个用来处理和生成大型数据集的编程模型和相关实现。用户需要指定*map*函数和*reduce*函数。*map*函数处理键值对并生成一组由键值对组成的中间值，*reduce*函数将所有键相同的中间值合并。就像本文中展示的那样，现实世界中的很多任务都可以通过这个模型表示。

以这种函数式风格编写的程序可以自动地作为并行程序在大型商用机集群上执行，运行时（run-time）系统负责对输入数据分区、在一系列机器间调度程序执行、处理机器故障、管理必要的机器间的通信。这让没有任何并行程序和分布式系统开发经验的编程人员能够轻松利用一个大型分布式系统的资源。

我们的MapReduce实现是高度可伸缩的，其运行在一个由商用机器组成的大型分布式集群上。通常，一个MapReduce计算会处理上千台机器上数TB的数据。每天都有数百个MapReduce程序提交的高达上千个MapReduce任务在Google集群上执行。开发人员认为这个系统非常易用。

## 1. 引言

在过去的五年中，本文作者和其他在Google的开发者实现了数以百计的计算程序，以计算处理不同来源的大规模原始数据（如爬取到的文档、web请求日志等）。这些程序可能用来计算倒排索引（inverted index）、web文档在图论中的各种表示、每个主机爬取到的页面数量之和、给定的某天中查询最频繁的集合等等。虽然大部分的计算程序逻辑非常简单，但是由于其输入数据的规模通常很大，所以这些程序必须在成百上千台机器上分布式执行以在可可接受的时间内完成。解决并行计算、数据分布、故障处理等问题需要大量复杂的代码，让原本简单的问题不再简单。

为了应对这种复杂性，我们设计了一个新的程序抽象。其允许我们通过简单的描述表达我们要执行的计算，同时将并行化、容错、数据分布、负载均衡等细节隐藏在库中。我们的抽象收到了Lisp和许多其他函数式语言中的*map*和*reduce*原语的启发。我们意识到，我们大部分的计算都设计*map*操作和*reduce*操作。首先对输入数据中每条逻辑记录应用*map*操作以计算出一系列的中间键值对，然后对所有键相同的值应用*reduce*操作以合理地整合这些派生数据。用户可以自定义*map*和*reduce*操作，这让大型计算的并行化更为简单，且可以使用“重跑（re-execution）”的方法作为主要容错机制。

本工作的主要贡献为一个简单且功能强大的能实现自动并行化、高伸缩性分布式计算的的接口，和该接口在大型商用PC集群上的高性能的实现。

[第二章](#2-)描述了基本编程模型，并给出了几个例子。[第三章](#3-)描述了为我们基于集群的计算环境定制的MapReduce接口实现。[第四章](#4-)描述了该编程模型中我们认为有帮助的细节。[第五章](#5-)我们的实现在各种任务重的性能测试。[第六章](#6-)探究了MapReduce在Google中的使用，其中包括了我们以MapReduce为基础重写我们产品索引系统的经历。[第七章](#7-)探讨了相关工作与未来的工作。

## 2. 编程模型

计算任务以一系列*输入键值对*作为输入，并产出一系列*输出键值对*作为输出。MapReduce库的用户将计算表示为两个函数：*map*和*reduce*。

用户编写的*map*函数将*输入键值对*处理为一系列*中间键值对*。MapReduce库将键相同的所有*中间键值对*的值与其对应的键$I$传递给*reduce*函数。

用户编写的*reduce*函数接收*中间键值对*的键$I$和该键对应的一系列值。它将这些值合并，并生产一个可能更小的一系列值。每个*reduce*函数调用通常产出0个或1个输出值。*中间键值对*中的值通过一个迭代器（iterator）供用户编写的*reduce*函数使用。这让我们能够处理因过大而无法放入内存中的值列表。

### 2.1 示例

考虑如下一个问题：统计一个大量文档集合中每个单词出现的次数。用户会编写如下的伪代码。

```
map(String key, String value):
  // key: document name
  // value: document contents
  for each word w in value:
    EmitIntermediate(w, "1");

reduce(String key, Iterator values):
  // key: a word
  // values: a list of counts
  int result = 0;
  for each v in values:
    result += ParseInt(v);
  Emit(AsString(result));
```

*map*计算出每个单词与其（译注：在每个文档中的）出现的次数（在本例中为“1”）。*reduce*函数会求出每个单词出现次数的和。

另外，用户编写代码来一个*mapreduce specification（规格/规范）*对象，填写输入输出文件名和可选的调节参数。随后，用户调用MapReduce函数，将*mapreduce specification*对象作为参数传入。用户代码会被与MapReduce库（C++实现）链接到一起。附录A包含本示例的完整程序。

### 2.2 类型

尽管前面的伪代码中使用了字符串作为输入输出类型，但理论上用户提供的*map*和*reduce*函数可以使用相关联的类型：

```
map     (k1,v1)        ->  list(k2,v2)
reduce  (k2,list(v2))  ->  list(v2)
```

即输入的键和值与输出的键和值的类型域不同，而中间键与值和输出键域值的类型与相同。

在我们的C++实现中，我们通过字符串将接受或传入用户定义的函数的参数，将字符串与适当类型的转换留给用户代码去实现。

### 2.3 更多示例

本节中，我们给出了一些简单的示例。这些示例是可以简单地通过MapReduce计算表示的有趣的程序。

- 分布式“grep”：如果一行文本匹配给定的模板，那么*map*函数会输出该行。*reduce*作为一个恒等函数，它仅将提供的中间数据复制到输出。

- URL访问频率计数：*map*函数处理web网页请求日志，并按照$<URL,1>$输出。*reduce*函数对$URL$相同的值求和，并输出$<URL,总数>$键值对。

- 反转web链接拓扑图：*map*函数对名为$source$的页面中每个名为$target$的URL链接输出一个$<target,source>$键值对。*reduce*函数按照所有$target$相同的$source$合并为一个列表，并与其相应的URL关联，输出$<target,list(source)>$键值对。

- 每个主机的词向量统计：词向量是对是对一个或一系列文档中最重要的词的总结，其形式为$<词,词频>$键值对列表。*map*函数为每篇输入文档输出一个$<主机名,词向量>$键值对（其中$主机名$由文档到的URL解析而来）。*reduce*函数会受到对于给定的主机上每篇文章的所有的词向量。其将这些词向量加在一起，丢弃掉低频词，并最终输出$<主机名,词向量>$键值对。

- 倒排索引：*map*函数对每篇文档进行提取，输出一个$<词,文档ID>$的序列。*reduce*函数接受给定词的所有键值对，并按照$文档ID$排序。输出一个$<词,list(文档ID)>$键值对。所有输出的键值对的集合组成了一个简单的倒排索引。如果需要持续跟踪词的位置，仅需简单的增量计算。

- 分布式排序：*map*提取每条记录中的键，输出一个$<键,记录>$的键值对。*reduce*函数不对中间变量作修改直接输出所有的键值对。排序计算依赖[章节4.1](#41-)中介绍的分区机制和[章节4.2](#42-)介绍的排序属性。

## 3. 实现

MapReduce接口可能有很多不同的实现。如何作出正确的选择取决于环境。例如，一种实现可能适合小型的共享内存的机器，一种实现可能适合大型NUMA多处理器主机，或者一种实现可能适合更大型的通过网络连接的机器集群。

本节中，我们将介绍一个中面向Google中常用的计算环境的实现。Google的常用计算环境为彼此通过以太网交换机连接的大型商用PC集群。在我们的环境中：

1. 机器通常使用双核x86处理器，2-4GB内存，运行Linux系统。

2. 使用商用网络硬件：每台机器带宽通常为100Mbps或1Gbps，但平均分到的带宽要小得多。（译注：可能受交换机间带宽限制，每台机器平均分到的带宽远小于其单机带宽。）

3. 一个集群由成百上千的机器组成，因此机器故障是常态。

4. 存储由直接连接到独立的机器上IDE（译注：本文IDE指集成设备电路Intergated Drive Electronics）磁盘提供。我们为了管理这些磁盘上的数据，开发了一个内部的分布式文件系统。该文件系统使用副本的方式在不可靠的硬件上提供了可用性和可靠性。

5. 用户将*job*提交到一个调度系统中。每个*job*由一系列的*task*组成，这些*task*被*scheduler（调度器）*映射到集群中一系列可用的机器上。

### 3.1 执行概览

输入数据会自动被分割为$M$个分片（split），这样，*map*函数调用可以在多个机器上分布式执行，每个输入的分片可以在不同机器上并行处理。*中间键值对*的键空间会通过被分区函数(例如，$hash(key) mod R$)分割为$R$个分区，这样，*reduce*函数也可以分布式执行。其中分区的数量（$R$）和分区函数由用户指定。

![图1 执行概览](figure-1.png "图1 执行概览")

**图1**展示了在我们的实现中，MapReduce操作的完整工作流。当用户程序调用MapReduce函数时会发生如下的操作（下列序号与图1中序号对应）：

1. 用户程序中的MapReduce库首先将输入文件划分为$M$个分片，通常每个分片为16MB到64MB（用户可通过可选参数控制）。随后，库会在集群中的机器上启动程序的一些副本。

2. 这些程序的副本中，有一份很特殊，它是master副本。其他的副本是被master分配了任务的worker副本。总计要分配$M$个*map task*和$R$个*reduce task*。master选取闲置的worker并为每个选取的worker分配*map task*或*reduce task*。

3. 被分配*map task*的worker从输入数据分片中读取内容。其解析输入数据中的键值对，并将每个键值对传给用户定义的*map*函数。*map*函数输出的*中间键值对*在内存中缓存。

4. 内存中缓存的键值对会定期地写入本地磁盘，写入的数据会被分区函数划分为$R$个区域。这些在磁盘中缓存的键值对的位置会被发送给master，master会将这些位置信息进一步传递给*reduce* worker。

5. 当master通知*reduce* worker*中间键值对*的位置信息后，*reduce* worker会通过远程过程调用（译注：即RPC。）的方式从*map* worker的本地磁盘中读取缓存的数据。当*reduce* worker读取完所有中间数据后，它会对中间数据按照键进行排序，以便将所有键相同的键值对分为一组。因为通常来说，需对键不同的数据会被映射到同一个*reduce task*中，所以需要对数据排序。如果中间数据总量过大以至于无法放入内存中，则会使用外排序算法（external sort）。

6. *reduce* worker遍历每一个遇到的*中间键值对*的，它会将键和该键对应的一系列值传递给用户定义的*reduce*函数。*reduce*函数的输出会被追加（append）到该*reduce*分区的最终输出文件中。

7. 当所有的*map task*和*reduce task*都执行完毕后，master会唤醒用户程序。此时，调用MapReduce的调应用序会返回到用户代码中。

在成功执行完毕后，MapReduce的输出可在通过$R$个输出文件访问（每个*reduce task*一个文件，文件名由用户指定）。通常情况下，用户不需要将这$R$个输出文件合并到一个文件中，用户经常将这些文件作为另一次MapReduce调用的输入，或者在另一个能够从多个分区的文件输入的分布式程序中使用这些文件。

### 3.2 master数据结构

master中保存着多种数据类型。对每个*map task*和*reduce task*，master会存储其状态（状态包括等待中（idle）、执行中（in-progress）和完成（conpleted））和非等待中的任务对应的worker的标识符。

master是将中间文件区域的位置从*map task*传递到*reduce task*的管道。因此，对于每个已完成的*map task*，master会存储其输出的$R$个中间文件区域的位置。当*map task*完成后=时，master会收到其对中间文件区域位置和大小信息的更新。这些信息会被增量地推送到有执行中的*reduce task*的worker中。

### 3.3 容错

因为MapReduce库是为使用成百上千台机器处理大规模数据提供帮助而设计的，所以必须能够优雅地对机器故障进行容错。

#### 3.3.1 worker故障

master会定期ping每个worker。如果在一定时间内没有收到worker的响应，master会将该worker标记为故障。被故障的worker处理的已完成的*map task*会被重设为其初始的“等待中”的状态，因此其符合被调度到其他worker的条件。同样，在故障的worker上任何执行中的*map task*或*reduce task*也会被重设为“等待中”的状态，符合重新调度的条件。

当worker故障发生时，该worker完成的*map task*也需要被重新执行，因为*map task*的输出被存储在故障的机器的本地磁盘上，无法被访问。故障worker完成的*reduce task*则不需要被重新执行，因为他们的输出被存储在全局的文件系统中

当一个起初被worker A执行的*map task*因A发生故障而随后被worker B执行时，所有正在执行*reduce task*的worker会被告知这个*map task*被重新执行。任何没从worker A中读取完数据的*reduce task*将会con欧冠worker B中读取数据。

MapReduce可以弹性处理大规模worker故障。例如，在MapReduce操作中，由于在正在运行的集群中的网络维护工作导致了80台机器在几分钟内同时变得不可访问。MapReduce的master会简单地重新执行不可访问的worker的机器上已完成的工作，并继续执行后续任务，最终完成珍格格MapReduce操作。

#### 3.3.2 master故障

我们让master简单地周期性地为之前提到的master中的数据结构设置检查点。如果master *task*挂掉，一份新的master的拷贝会从最后一次检查点的状态重启。尽管只有一个master，发生故障的可能性也很小。因此，目前我们的实现方式为：如果master故障，则终止MapReduce计算。client可以检测到该状态，如果有需要可以重试MapReduce操作。

#### 3.3.3 故障出现时的语义

对于相同的输入数据，当用户提供的*map*和*reduce*操作是确定性函数时（译注：确定性函数指在任何时候，当函数输入相同时，总会得到相同的输出。），分布式的MapReduce输出的数据和一个没发生故障的顺序执行的程序输出的数据是一样的。

我们通过原子性地提交*map task*和*reduce task*输出的方式来实现这一性质。每个执行中的*task*将其输出写入到私有的临时文件中。每个*reduce task*会创建一个这样的临时文件，每个*map task*会创建$R$个这样的临时文件（每有一个*reduce task*就创建一个）。当有一个*map task*完成时，该worker会向master发送一条带有$R$个临时文件名的消息。如果master收到了一个已经完成过的*map task*的完成消息，master会忽略该消息。否则，master会在其数据结构中记录这$R$个文件的文件名。

当有一个*reduce task*完成时，该worker会自动地将其临时输出文件重命名为一个永久的文件名。如果同一个*reduce task*被在多台机器中执行，会出现多个重命名调用将文件重命名同一个永久文件名的情况。我们依赖下层文件系统提供了原子性重命名操作，来保证最终的文件系统中仅包含来自一次*reduce task*输出的数据。

我们绝大多数*map*和*reduce*操作是确定性的。因此，分布式的MapReduce语义等同于顺序执行的语义。这使得编程人员可以很容易地理解程序行为。当*map*和（或）*reduce*为非确定性函数时，我们提供了较弱但仍合理的语义。当非确定性的操作出现时，一次特定的*reduce task*的输出$R_{1}$等同于这个非确定性操作顺序执行的输出$R_{1}$。但是，不同次*reduce task*的输出$R_{2}$可能对应这个非确定性操作顺序不同次执行的输出$R_{2}$。

考虑这样一种情况，有*map task* $M$和*reduce task* $R_{1}$和$R_{2}$。$e(R_{i})$表示被提交的任务$R_{i}$的执行过程（有且仅有一个该执行过程）。因为$e(R_{1})$与$e(R_{2})$可能读取了任务$M$的不同次执行后的输出文件，因此会出现较弱的语义。（译注：即如果$M$因故障等原因被多次执行，因为$M$多次执行的输出不一致，所以$R_{1}$和$R_{2}$读取的输入可能不一致。）

### 3.4 位置分配

在我们的计算环境中，网络带宽是相对稀缺的资源。为了节约网络带宽，我们将输入数据（由GFS管理）存储在集群中机器的本地磁盘中。GFS将每个文件分割为若干个64MB的块，并为每个块存储在不同机器上若干个副本（通常为3个）。MapReduce的master会考虑输入文件的位置信息，并试图在持有输入文件的副本的机器上分配相应的*map task*。如果分配失败，master会试图将*map task*分配在离其输入文件的副本较近的机器上（例如，在与持有输入数据副本的机器在相同交换机下的机器上分配）。在集群中较大比例的机器上运行大型MapReduce操作时，大部分输入数据都是从本地读取，不消耗网络带宽。

### 3.5 任务粒度

如前文所述，我们将*map*阶段进一步划分为$M$份，将*reduce*阶段进一步划分为$R$份。在理想状态下，$M$和$R$应远大于worker的机器数。让每个worker执行多个不同的*task*可以提高动态负载均衡能力，也可以在一个worker故障时提高恢复速度：该worker完成的多个*map task*可以被分散到所有其他的worker机器上执行（译注：否则，考虑$M$小于worker机器数的情况，每个worker上只有一个任务，如果一个worker故障，那么该worker中完成的任务只能在另一台worker机器上重跑，无法充分利用并行的性能）。

在我们的MapReduce实现的实际情况中，对$M$和$R$的上限进行了限制。如前文所述，master必须做出$O(M+R)$个调度决策，并在内存中保存$O(M \times R)$个状态。（内存占用的常数因子比较小：$O(M \times R)$条状态由大约每个*map*/*reduce*任务仅一字节的数据组成。）

此外，$R$还经常受用户限制，因为每个*reduce task*会生成一个单独的输出文件。在实际情况下，我们更倾向于自定义参数$M$，这样可以使每个单独的*task*的输入数据大概在16MB到64MB（这样可以使前面提到的局部性优化最有效），同时，我们使$R$是期望使用的worker机器的较小的倍数。我们经常在$2,000$台机器上选择$M=200,000$、$R=5,000$的参数执行MapReduce计算。

### 3.6 任务副本

延长MapReduce操作总时间的常见原因之一为“离群问题”：一个机器花费了不寻常的长时间完成计算中最后的几个*map task*或*reduce task*。吃线离群问题的原因有很多。例如，一台磁盘情况不良的机器可能频繁修正磁盘错误，导致其读取速度从$30MB/s$降低到$1MB/s$。集群的调度系统可能已经将其他任务调度到了该机器上，导致其因CPU、内存、本地磁盘或网络带宽等因素执行MapReduce代码更慢。我们最近遇到的问题是在机器初始化代码中的一个bug，其导致了处理器缓存被禁用，受影响的机器上的计算慢了超过100倍。

我们有一个通用的机制来避免离群问题。当MapReduce操作将要完成时，master会通过调度对仍在执行中的任务创建副本并执行。当原任务和其副本之一执行完成时，该任务会被标记为已完成。我们对这个机制进行了一些调优，使它通常情况下对计算资源的占用仅提高几个百分点。我们发现这个机制显著地减少了完成大型MapReduce操作的时间。例如，[章节5.3](#53-)中的排序程序在禁用任务副本机制时，完成时间延长了44%。

## 4. 细节

