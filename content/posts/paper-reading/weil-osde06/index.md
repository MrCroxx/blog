---
title: "《Ceph: A Scalable, High-Performance Distributed File System》论文翻译 [持续更新中]"
date: 2020-09-14T17:38:45+08:00
lastmod: 2020-09-14T17:38:45+08:00
draft: false
keywords: []
description: ""
tags: ["Ceph", "Translation"]
categories: ["Paper Reading"]
author: ""
resources:
- name: featured-image
  src: paper-reading.jpg
---

*本篇文章是对论文[weil-osdi06](https://www.usenix.org/legacy/events/osdi06/tech/full_papers/weil/weil_html/)的原创翻译，转载请严格遵守[CC BY-NC-SA协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)。*


<!--more-->

## 摘要

我们开发了分布式文件系统——Ceph，其能提供极好的性能、可靠性和伸缩性。Ceph使用为不可靠的对象存储设备（unreliable object storage devices，OSD）的异构动态集群设计的伪随机数据分布函数（pseudo-random data distribution function）CRUSH来替代分配表（allocation），从而最大化地分离了数据管理与元数据管理。我们通过数据副本、故障检测、和恢复的方式，将设备的智能<sup>译注1</sup>应用到运行专用本地对象文件系统的半自治OSD中。动态的分布式元数据集群提供了极为高效的元数据管理能力，并适用于大部分通用文件系统和科学计算文件系统的负载。从在各种负载下的性能测试中可以得出，Ceph有着极好的I/O性能和可伸缩的元数据管理能力，能够支持超过每秒25000次元数据操作。

> 译注1：这里的智能（intelligence）指OSD设备上的CPU与内存能够提供的能力，在后文中有提到。

## 1. 引言

系统长期以来一直在追求改进文件系统的性能，文件系统的性能已被证明对极多类型的应用程序的整体性能至关重要。科学计算和高性能计算社区推动了分布式存储系统在性能和伸缩性的提升，这比更通用的需求提前几年。如NFS<sup>[20]</sup>等传统解决方案提供了一个简单地模型，即服务器导出文件系统层次，客户端可将这个文件系统层次映射到它们的本地命名空间中。尽管这种方式被广泛使用，这种client/server模型中的中心化设计已被证明对可伸缩性的表现有很大阻碍。

更近一些的分布式存储系统采用了基于对象存储的架构，这种架构将传统的硬盘被替换为智能对象存储设备（OSD），OSD合并了CPU、网络接口、和有底层磁盘或RAID的本地缓存<sup>[4, 7, 8, 32, 35]</sup>。OSD用一个新的接口取代的传统的块级接口，客户端可以通过这个接口读写更大（且通常大小不一）的命名对象中的字节区间，并将低级块分配决策留给了设备本身。客户端通常与以元数据服务器（metadata server，MDS）交互的方式来执行元数据操作（如open、rename），而直接与OSD通信来以执行文件I/O（read、write），这大改善了整体的可伸缩性。

采用这种模型的系统由于很少或没有分发元数据的负载，其还是会受可伸缩性限制的困扰。而继续依赖传统文件系统的原则（如分配列表和inode列表）且不愿将智能委托给OSD进一步限制了可伸缩性和性能，且增加了可靠性的开销。

因此，我们提出了Ceph，Ceph是一个提供了极好的性能与可靠性且具有无可比拟的可伸缩性的分布式文件系统。我们的架构基于如下的假设：PB级别的系统本质上是动态的————大型系统不可避免的需要增量构建、节点故障是常态而不是意外、负载的量和特征会随着时间不断改变。

Ceph通过使用生成函数来替代文件分配表的方式，将数据操作与元数据操作解耦。这让Ceph能够利用OSD已有的智能来分散数据访问、串行更新、副本与可靠性、故障检测和恢复的复杂性。Ceph采用了高适用能力的分布式元数据集群架构，这极大地提高了元数据访问的可伸缩性，并因此提高了整个系统的可伸缩性。我们将讨论那些驱动我们选择了这种架构的目标与负载假设、分析它们对系统可伸缩性和性能的影响、并与我们在实现功能性系统原型的经验相结合。

## 2. 系统总览

Ceph文件系统有3个主要部件：（1）客户端：为主机或进程暴露类POSIX文件系统接口的实例；（2）OSD集群：共同存储所有的数据与元数据；（3）元数据服务器集群：管理命名空间（文件名与目录），同时协调安全性（security）、一致性（consistency）和连续性（coherence）。如**图1**所示。我们称Ceph接口是类POSIX接口，因为我们为了更好地与应用程序的需求对齐并改进系统性能，我们对POSIX接口进行了适当的扩展，并选择性地放松了一致性语义。

![图1 系统架构。客户端通过直接与OSD通信来执行文件I/O。每个进程既可以直接链接到客户端实例，也可以通过一个挂载的文件系统交互。](figure-1.png "图1 系统架构。客户端通过直接与OSD通信来执行文件I/O。每个进程既可以直接链接到客户端实例，也可以通过一个挂载的文件系统交互。")

该架构的主要目标是伸缩性（用于数百PB或更多）、性能、和可靠性。伸缩性被从多维度考虑，包括系统整体的存储容量、吞吐量、和单个客户端、目录或文件的性能。我们的目标负载可能包括一些极端情况，如数万或数十万个主机并发地读取或写入同一个文件，或在同目录下创建文件。这种场景在运行在超级计算机集群上的科学计算程序中很常见，并在未来的通用的负载中越来越多地起着指示作用。更重要的是，我们发现分布式文件系统的负载本质上是动态的，随着活动的应用程序和数据集随着时间的变化，数据与元数据的访问也显著的变化。Ceph直接解决了伸缩性的问题，同时通过三个基本设计特性解决了高性能、可靠性和可用性，这三种设计为：数据与元数据解耦、动态分布式元数据管理、和可靠的自主分布式对象存储。

**数据与元数据解耦：** Ceph将文件元数据与管理与文件数据的存储最大化地进行了分离。元数据操作（如open、rename等）被元数据集群共同管理，同时，客户端直接与OSD交互来执行文件I/O（读取和写入）。基于对象的存储通过将低级的块分配决策交给单个设备以提高文件系统的可伸缩性。然而，与现有的基于对象的文件系统<sup>[4, 7, 8, 32]</sup>不同，Ceph使用更短的对象列表替代较长的每个文件的块列表，因此完全消除了分配列表。文件数据被分条（strip）为可预测的命名对象上，同时通过一个叫CRUSH<sup>[29]</sup>的专用数据分布函数将对象分配到存储设备上。这让任一方都可以计算（而不是查找）组成文件内容的对象的名称与位置，消除了维护与分发对象列表的需求，简化了系统设计，减少了元数据集群的负载。

**动态分布式元数据管理：** 因为文件系统元数据操作组成了传统文件系统负载的一半<sup>[22]</sup>，高效的元数据管理对整个系统的性能非常重要。Ceph使用了一种基于动态子树分区（Dynamic Subtree Partitioning）<sup>[30]</sup>的新式元数据集群架构，并智能地将管理文件系统目录层级的责任分布到数十甚至数百个MDS上。（动态的）分层分区在每个MDS的负载都保持了局部性，这促进了高效地更新和主动式预取来改进常见的负载的性能。重要的是，元数据服务器间的负载分布完全基于当前的访问模式，使Ceph在任何负载下都能够高效利用可用的DMS资源，并实现与MDS数量呈近线性的伸缩性。

**可靠的自主分布式对象存储：** 由成千上万个设备组成了大型系统本质上是动态的：它们是被增量构建的，当部署新存储或退役旧设备时，它们会跟着伸展或收缩，且大量的数据会被创建、移动、和删除。所有的这些因素都要求数据的分布变得能够高效利用可用资源并维护所需的数据副本等级。Ceph把数据迁移、复制、故障检测、和故障恢复的责任托付给了存储数据的OSD集群，同时在上层，OSD共同为客户端和元数据服务器提供一个逻辑对象存储。这种方法让Ceph能够更高效地利用每个OSD的智能（CPU和内存）来实现可靠的、高可用的、有着线性伸缩性的对象存储。

本文中，我们描述了Ceph的客户端的操作、元数据服务器集群、和分布式对象存储，还描述了我们的架构怎样影响它们的关键特性。我们还描述了我们原型的状态。

## 3. 客户端操作

我们将介绍Ceph组件的整体操作，并通过描述Ceph的客户端的操作的方式介绍它们与应用程序的交互。Ceph的客户端运行在每个执行应用程序代码的主机上，并为应用程序暴露文件系统接口。在Ceph原型中，完全运行在用户空间，且既可以直接链接到它来访问，又可以通过FUSE（一种用户空间文件系统接口）<sup>[25]</sup>作为挂载的文件系统访问。每个客户端维护它拥有的文件数据缓存，该缓存与内核也或缓冲区缓存独立，使直接链接了客户端的应用程序能够访问它。

### 3.1 文件I/O和功能

当进程打开一个文件时，客户端会向MDS集群发送一个请求。MSD会遍历文件系统的层次结构，将文件名转换为文件的inode，inode中包括唯一的inode编号、文件所有者、模式、大小、和其他的单文件元数据。如果文件存在并有访问权限，MDS会返回inode编号、文件大小和将文件数据映射到对象的分条策略的信息。MDS可能还会向客户端发送（如果其还没收到过）指定了哪些操作是被允许的功能（capability）信息。目前，这些功能包括4位，其控制客户端的读（read）、缓存读（cache reads）、写（write）、和缓冲写（buffer writes）的能力。在未来，其功能还将包括允许客户端向OSD证明其被授权了读写数据权限的安全密钥<sup>[13, 19]</sup>（目前，原型信任所有客户端）。后续操作中，MDS在文件I/O中的参与仅限于管理功能，以维护文件的一致性并实现适当的语义。

Ceph有很多将文件数据映射到一系列对象上的分条策略。为了避免任何对与新文件分配（allocation）相关元数据的需求，对象名简单地将文件inode编号和条带（stripe）号。接着，会使用CRUSH将对象的副本分配到OSD上，CRUSH是全局可知的映射函数（在[章节5.1](#51-)中介绍）。例如，如果一个或多个客户端打开了一个文件用来读取，MDS会为它们授权读取和缓存文件内容的功能。有了inode编号、布局、和文件大小，客户端可以命名并定位所有包含文件数据的对象，并能够直接从OSD集群读取。任何不存在的对象或字节区间被定义为文件的“洞”，或者“零值”。类似地，如果客户端打开了文件用来写入，它会被授权带缓存的写入的功能，其在该文件的任何偏移量上生成的任何数据会被简单地写入到适当OSD上的适当对象中。客户端在文件关闭时会放弃（relinquish）对应的功能，并向MDS提供新文件的大小（写入的最大偏移量），这会重新定义（可能）已有的包含文件数据的一系列对象。

### 3.2 客户端同步

POSIX要求读取操作能够反映之前写入的任何数据，且写入操作时原子性的（也就是说，重叠的并发写入将会反映一个特定的写入顺序）。当文件被多个客户端打开时（多个writer或既有writer又有reader），MDS将会收回之前任何的缓存读取和缓冲区写入功能，强制同步客户端对该文件I/O。这样，每个应用程序的读取或写入操作将会被阻塞，直到OSD确认，这增加了OSD中存储的每个对象的串行更新和同步的负担。当写入跨对象边界时，客户端会请求对受影响的对象的独占锁（由这些对象所在的各自OSD授权），并立即提交写入并解锁操作，以实现所需的串行性。对象锁还可被用于在大型写入时，通过获取锁并异步冲刷（flush）数据来掩盖延迟。

意料中的是，同步I/O对应用程序的性能有很大的影响，特别是对那些进行小规模读写的应用程序来说更加明显，这时延迟造成的，其至少需要与OSD的一次往返的延迟。尽管在通用的负载中，读写共享的情况相对比较少<sup>[22]</sup>，但是在可写计算应用程序中，这种场景是非常常见的<sup>[22]</sup>，且这种情况下性能通常很重要。因此，当应用程序不需要依赖严格的标准一致性时，通常希望能够放松一致性来减少开销。尽管Ceph支持通过全局的开关来放松一致性，正如许多其他分布式文件系统在该问题上做的一样<sup>[20]</sup>，但是这是一种不精确且不能令人满意的觉接方案：要么性能会下降，要么会在系统范围下丢失一致性。

正由于这个原因，高性能计算（high-performance computing，HPC）社区<sup>[31]</sup>提出了一系列的对POSIX I/O的高性能计算扩展接口，Ceph中实现了这些接口中的一个子集。其中最引人注意的是，open操作的`O_LAZY`标识符允许应用程序显式地放松对共享写文件通常的连续性要求。管理自己连续性（例如HPC负载中常见的模式，通过写入同一个文件的不同部分）的性能敏感型程序在执行I/O时就可以通过缓冲区写入或通过缓存读取，否则只能同步执行。如果需要，应用程序可以进一步显式地通过两种额外的调用进行同步：`lazyio_propagate`会将给定的字节区间冲刷到对象存储中、`lazyio_synchronize`会确保过去的修改会在任何后续的读取中反映。因此，为了保持Ceph同步模型的简单性，其在客户端间通过同步I/O提供正确的*读-写*和*共享写*语义，并扩展了应用程序接口来放松性能敏感的分布式程序的一致性。

### 3.3 命名空间操作

客户端与文件系统命名空间的交互由元数据服务器集群管理。读操作（如readdir、stat）和更新（如unlink、chmod）都由MDS同步应用，以确保串行、一致性、正确的百密性（correct security）、和安全性（safety）。为了简单起见，客户端不使用元数据锁或租约。特别是对于HPC的负载，回调能够提供好处很小，但复杂性的潜在开销很高。

相反，Ceph为大多数通用元数据访问场景做了优化。在readdir之后对每个文件执行stat（例如，`ls -l`）是一个非常常见的访问模式，且是在大目录下臭名昭著的性能杀手。Ceph中的readdir仅需一次MDS请求，它会拉取整个目录。包括inode的内容。默认情况下，如果readdir后面会立刻接一个或多个stat，那么被缓存的简短的信息会被返回；否则，缓存的信息就被丢弃。虽然这种方法在中间inode修改可能不会被注意到的情况下稍稍放松了连续性，但是我们还是十分乐于通过这种交换来大幅改进性能。这种行为可被readdirplus<sup>[31]</sup>扩展显式地捕捉到，它会返回整个目录的lstat的结果（正如再一些专用OS的实现中getdir已经做的那样）。

Ceph可以通过更久地缓存元数据来允许一致性被进一步放松，这很像早期版本的NFS做的那样，其通常缓存30秒。然而，这种方法会在某种程度上打破连续性，通常这对应用程序来说是很重要的，比如那些使用stat来判断一个文件是否被更新过的应用程序。如果这样做，这些应用程序可能会执行不正确的行为，或要等待旧的缓存值超时。

我们再次选择了提供正确的行为并扩展了对性能有不利影响的接口。这种选择可通过下例清楚的说明：对一个被多个客户端并发为写入而打开的文件的stat操作。为了返回正确的文件大小和修改时间，MDS会收回任何写入的功能，以立刻停止更新并采集最新的大小和所有writer的修改时间。其中最大值会被返回，随后被撤销的功能会被重新下发以执行后续进程。尽管停止多个writer似乎有些过于激进，但为了保证合理的串行化，这时必需的。（对于单个writer，可以从正在写入的客户端检索到正确值，而不需要打断进程。）不需要连续性行为的应用程序（也就是需求与POSIX接口不一致的受害者）可以使用statlite<sup>[31]</sup>，其通过一个位掩码来执行哪些inode字段不需要连续性。

## 4. 动态分布式元数据

元数据操作经常会占用文件系统一半的负载<sup>[22]</sup>且操作位于关键路径中，这对MDS集群的整体性能来说是至关紧要的。元数据管理也成为了分布式文件系统中伸缩性的重要挑战：尽管在增加更多存储设备时，容量和总I/O速率几乎可以任意伸缩，但是元数据操作设计更大程度的相互依赖关系，这使可伸缩的一致性与连续性管理变得更加困难。

Cpeh中的文件和目录的元数据非常小，其由几乎整个目录里的条目（文件名）和inode（80B）组成。不像传统的文件系统，Ceph中没有文件分配（allocation）元数据是必要的——对象名由inode号组成，并通过CRUSH分布到OSD中。这简化了元数据负载并使我们的MDS能够高效的管理大量的文件，无论文件有多大。我们的设计通过双层存储策略（two-tiered storage strategy）进一步追求减少元数据相关的磁盘I/O，并最大化局部性（locality），且通过动态子树分区（Dynamic Subtree Partitioning）<sup>[30]</sup>来高效利用缓存。

### 4.1 元数据存储

尽管MDS集群的目标是通过其内存缓存来满足大部分请求，但是为了安全起见，元数据的更新必须被提交到磁盘上。一系列大量、有界、懒惰冲刷的日志（journal）让每个MDS可以高效、分布式地将其更新的元数据流式写入到OSD集群中。每个MDS的几百MB的日志中也会有重复的元数据更新（在大多数负载中很常见），因此当旧的入职条目最终被冲刷到长期存储时，许多条目已经被废弃了。尽管MDS的恢复目前还没在我们的原型中实现，日志还是按照如下功能设计的：当一台MDS故障时，另一台节点可以快速重新扫描日志并恢复故障节点内存缓存中的重要内容，以恢复文件系统的状态。

这种策略是两全其美的：可以高效（顺序）地流式更新到磁盘，且大大较少了重写的负载，这允许长期磁盘存储的布局可对未来的读访问进行优化。特别是，inode被直接嵌入到了目录中，这让MDS可以通过单词OSD读请求预拉取整个目录，并高度利用大部分负载中的目录局部性<sup>[22]</sup>。每个目录的内容会使用与元数据日志和文件数据相同的分条和分布策略写入OSD集群中。inode编号会按照一定范围分配给元数据服务器，且在我们的原型中inode编号被认为是不可变的，尽管之后在文件删除时它们可能会被简单地回收。辅助锚表（auxiliary anchor table）用来保存很少见的有多个硬链接的可通过编号全局寻址的inode，所有这些都不需要使用非常常见的有庞大、稀疏且笨重的inode表的单链接文件。

### 4.2 动态子树分区

我们的主拷贝缓存策略（primary-copy caching strategy）让一个权威的MDS负责管理对任意给定元数据片段的缓存一致性和串行更新。大部分现有的分布式文件系统使用某种基于子树的静态分区的形式来授权（通常强迫管理员将数据集分割为更小的静态“卷”），一些最近的实验性文件系统使用了哈希函数来分布目录和文件元数据<sup>[4]</sup>，这为分摊负载而牺牲了局部性。这两种方法都有严重的局限性：静态子树分区无法应对动态的负载和数据集，而哈希破坏了元数据的局部性和实现高效元数据预拉取与存储的重要的可能性。

![图2 Ceph动态地将目录层级的子树根据当前的负载映射到元数据服务器中。每个独立的目录仅当它们成为热点时，才会被通过哈希分布到多个节点中。](figure-2.png "图2 Ceph动态地将目录层级的子树根据当前的负载映射到元数据服务器中。每个独立的目录仅当它们成为热点时，才会被通过哈希分布到多个节点中。")

Ceph的MDS集群基于一种动态子树分区策略<sup>[30]</sup>，其在一组节点间自适应地分层分布缓存元数据，如**图2**所示。每个MDS会通过随时间指数衰减的计数器来测量每个目录层级的元数据的流行度（popularity）。任何操作都会增长受影响的inode和从它们开始向上直到根目录所有计数器，这为每个MDS提供了描述最近负载分布的加权树。每个一段时间，MDS的负载值会被进行比较，且目录层级的适当大小的子树会被迁移，以保证负载最终会被分布。使用共享的长期存储和谨慎构造的命名空间锁，让这种迁移可以通过将内存缓存中的适当的内容传输到新的被授权的节点实现，这样可以减少对连续性锁或客户端功能的影响。为了安全起见，导入的元数据会被写入新MDS的日志中，且两边的额外日志确保了授权的转移不会受中间发生的故障影响（类似于两段式提交）。最终得到的基于子树的分区会保持粗粒度，以减少前缀复制开销并保持局部性。

当元数据被复制到多个MDS节点时，inode的内容会被划分为三组，每组都有不同的一致性语义：安全组（所有者、模式）、文件组（大小、修改时间）、和不可变组(inode编号、创建时间、布局)。不可变组的字段永远不会改变，而安全组合文件组会被单独的有限状态机管理，每个有限状态机都有不同的一系列状态和转移，这样的设计是为了在减少锁的争用的同时适应不同的访问和更新模式。例如，在路径遍历时，会需要对“所有者”和“模式”进行安全检查，但二者很少变化，仅需很少的状态；而因文件锁控制MDS发送给客户端的功能，其反映了更广的客户端访问模式，

### 4.3 流量控制

将目录层级在多台节点上分区可以均衡负载，但是不是总能解决热点（hot sopt）和瞬时拥堵（flash crowd）问题，在这些问题中，许多客户端会访问同一个目录或文件。Ceph通过它对元数据流行度的了解，将热点宽泛地分布，这仅在需要时执行，且一般情况下不会曹正相关的额外开销和目录局部性的损失。被大量读取的目录（例如被多个open打开）的内容会被有选择地在多个节点上备份以分布负载。特别大或者有大量写入负载的目录（如目录有许多文件的创建）的内容会按文件名哈希并分布在集群中，以牺牲目录的局部性来换取负载的平衡。这种自适应的方法让Ceph适用于各种分区粒度，让文件系统在特定环境下的不同部分能够有最有效的分区粒度策略，让系统能同时获得粗粒度和细粒度带来的好处。

每个MDS的响应都为客户端提供了有关授权和相关inode和其任何副本及祖先inode的更新后的信息，让客户端了解文件系统中与客户端交互的部分的元数据分区。之后的元数据操作将会基于给定路径中已知的最深前缀，直接与被授权的结点进行（对于更新操作），或者与随机一份副本进行（对于读取操作）。通常，客户端会得知不流行（没被做副本的）的元数据的位置，且可以直接与适当的MDS通信。而对于访问流行元数据的客户端，它们会被告知元数据贮存在不同的或多个MDS节点上，这可以有效限制认为元数据中任一部分贮存在任一MDS上的客户端的数量，这样可以在潜在的热点和瞬时拥堵产生前分散负载。

## 5. 分布式对象存储

从上层来看，Ceph的客户端和元数据服务器视对象存储集群（可能有上万或上十万个OSD）为一个对象存储和命名空间。Ceph的可靠自主分布式对象存储（Reliable Autonomic Distributed Object Store，RADOS）在容量和整体性能方面实现了线性伸缩性，这是通过将对象备份、集群扩展、故障检测和恢复等方面的管理分布式地授权给OSD实现的。

### 5.1 使用CRUSH分布数据

Cpeh必须将PB级的数据分布到由数千个存储设备组成的不断演进的集群上，这样可以有效地利用设备存储和带宽资源。为了避免不平衡（例如，最近部署的设备几乎是空闲或空的）或负载不对称（例如，新的、热点的数据仅在新设备上），我们使用了这样一种策略：随机分布新数据、对已有数据随机二次抽样并迁移到新设备、均匀地重分配被移除的设备中的数据。这种随机的方法具有鲁棒性，其可以在任何潜在的负载中有同样好的表现。

![图3 文件被分条为许多对象，且被分组到放置组（placement group，PG）中，并通过专用的副本放置函数——CRUSH分布到OSD上。](figure-3.png "图3 文件被分条为许多对象，且被分组到放置组（placement group，PG）中，并通过专用的副本放置函数——CRUSH分布到OSD上。")

Ceph首先使用一个简单的哈希函数将对象映射到放置组（placement group，PG）中，该哈希函数中有一个可调的位掩码，以控制PG的数量。我们选定的值给每个OSD大约100个PG，来通过平衡每个OSD维护的副本相关的元数据的总量，以平衡OSD利用率的差异。接着，会将放置组通过CRUSH算法（Controlled Replication Under Scalable Hashing）分配给OSD，CRUSH是一个伪随机数据分配函数，其可以高效地将每个PG映射到一个有序的存储对象副本的OSD列表上。这与传统的方法（包括其他的基于对象的文件系统）的不同之处在于，数据分配不依赖任何块或对象列表元数据。为了定位任意对象，CRUSH仅需要放置组和OSD集群映射，二者是对组成存储集群的设备的紧凑、分层的描述。这种方法有两个关键优势：首先，这是完全分布式的，任何一方（客户端、OSD或MDS）可以独立地计算任何对象的位置；第二，映射不会频繁更新，这消除了任何与数据分布相关的元数据交换。这样做，CRUSH同时解决了数据分布问题（“我该把数据存在哪儿”）和数据定位问题（“我把数据存到哪儿了”）。按照设计，对存储集群的较小的修改对已有的PG映射影响非常小，这减少了由于设备故障或集群扩展而导致的数据迁移。

集群映射的结构层次被构造为与集群的物理或逻辑组成和潜在的故障源保持一致。例如，用户可以为一个由满是OSD的机架格（shelf）、满是机架格的机架（rack cabinet）、和多行机架（row of cabinet）组成的设备构造一个4层的映射。每个OSD还有一个权值，用来控制分配给它的相对的数据总量。CRUSH会基于放置规则（placement rule）把PG映射到OSD，放置规则定义了副本级别和任何放置上的约束。例如，用户可能想把每个PG在3个OSD上做副本，且所有副本都在同一行机架（以限制机架行间的备份流量）但位于不同的机架上（以减少电源电路或边缘开关故障带来的影响）。集群映射还包括一个离线或非活跃设备的列表和一个时期号（epoch number），每次映射变化时该时期号会增加。所有OSD请求会被打上客户端映射的时期号的标签，这样，所有方会对当前的数据分布达成一致。增量的映射更新会在协作的OSD间共享，如果映射更新，OSD的回复中也会带上这个数据。

### 5.2 副本

与像Lustre<sup>[4]</sup>这样的系统不同，Lustre假设用户可以在SAN上使用RAID或故障转移机制（fail-over）来构建足够可靠的OSD，而我们假设PB或EB的系统中故障是正常时间而非异常事件，且在任意时间点都会有几个OSD可能无法使用。为了维护系统的可用性并在系统伸缩时仍能保证数据安全，RADOS使用一种主拷贝备份（primary-copy replication）的变体<sup>[2]</sup>来管理其自己的数据副本，同时采取措施以减少对性能的影响。

数据被按照放置组备份，每个放置组被映射到一个由n个OSD组成的有序列表上（为了n路备份）。客户端将所有的写请求发送到对象的PG中第一个没有故障的OSD中（主OSD，primary OSD），其会为该对象和PG分配一个新的版本号，并将写请求进一步传递给所有其他的备OSD（replica OSD）上。在每个副本都应用了更新并响应主OSD后，主OSD会将更新应用到本地并通知客户端这次写入。读请求会被定向到主OSD。这种方法为客户端省去了副本间同步或序列化所带来的复杂性，在有其他writer或故障恢复时，这会变得很麻烦。这种方法还会将做副本消耗的带宽从客户端转移到OSD集群内部网络中，在我们的期望中，OSD集群内部网络会有更多的可用资源。中间的备OSD故障会被胡烈，因为任何的后续恢复操作（[章节5.5](#55-)）都将可靠地恢复副本的一致性。

### 5.3 数据安全性

