---
title: "《Ceph: A Scalable, High-Performance Distributed File System》论文翻译 [持续更新中]"
date: 2020-09-14T17:38:45+08:00
lastmod: 2020-09-14T17:38:45+08:00
draft: false
keywords: []
description: ""
tags: ["Ceph", "Translation"]
categories: ["Paper Reading"]
author: ""
resources:
- name: featured-image
  src: paper-reading.jpg
---

*本篇文章是对论文[weil-osdi06](https://www.usenix.org/legacy/events/osdi06/tech/full_papers/weil/weil_html/)的原创翻译，转载请严格遵守[CC BY-NC-SA协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)。*


<!--more-->

## 摘要

我们开发了分布式文件系统——Ceph，其能提供极好的性能、可靠性和伸缩性。Ceph使用为不可靠的对象存储设备（unreliable object storage devices，OSD）的异构动态集群设计的伪随机数据分布函数（pseudo-random data distribution function）CRUSH来替代分配表（allocation），从而最大化地分离了数据管理与元数据管理。我们通过数据副本、故障检测、和恢复的方式，将设备的智能<sup>译注1</sup>应用到运行专用本地对象文件系统的半自治OSD中。动态的分布式元数据集群提供了极为高效的元数据管理能力，并适用于大部分通用文件系统和科学计算文件系统的负载。从在各种负载下的性能测试中可以得出，Ceph有着极好的I/O性能和可伸缩的元数据管理能力，能够支持超过每秒25000次元数据操作。

> 译注1：这里的智能（intelligence）指OSD设备上的CPU与内存能够提供的能力，在后文中有提到。

## 1. 引言

系统长期以来一直在追求改进文件系统的性能，文件系统的性能已被证明对极多类型的应用程序的整体性能至关重要。科学计算和高性能计算社区推动了分布式存储系统在性能和伸缩性的提升，这比更通用的需求提前几年。如NFS<sup>[20]</sup>等传统解决方案提供了一个简单地模型，即服务器导出文件系统层次，客户端可将这个文件系统层次映射到它们的本地命名空间中。尽管这种方式被广泛使用，这种client/server模型中的中心化设计已被证明对可伸缩性的表现有很大阻碍。

更近一些的分布式存储系统采用了基于对象存储的架构，这种架构将传统的硬盘被替换为智能对象存储设备（OSD），OSD合并了CPU、网络接口、和有底层磁盘或RAID的本地缓存<sup>[4, 7, 8, 32, 35]</sup>。OSD用一个新的接口取代的传统的块级接口，客户端可以通过这个接口读写更大（且通常大小不一）的命名对象中的字节区间，并将低级块分配决策留给了设备本身。客户端通常与以元数据服务器（metadata server，MDS）交互的方式来执行元数据操作（如open、rename），而直接与OSD通信来以执行文件I/O（read、write），这大改善了整体的可伸缩性。

采用这种模型的系统由于很少或没有分发元数据的负载，其还是会受可伸缩性限制的困扰。而继续依赖传统文件系统的原则（如分配列表和inode列表）且不愿将智能委托给OSD进一步限制了可伸缩性和性能，且增加了可靠性的开销。

因此，我们提出了Ceph，Ceph是一个提供了极好的性能与可靠性且具有无可比拟的可伸缩性的分布式文件系统。我们的架构基于如下的假设：PB级别的系统本质上是动态的————大型系统不可避免的需要增量构建、节点故障是常态而不是意外、负载的量和特征会随着时间不断改变。

Ceph通过使用生成函数来替代文件分配表的方式，将数据操作与元数据操作解耦。这让Ceph能够利用OSD已有的智能来分散数据访问、串行更新、副本与可靠性、故障检测和恢复的复杂性。Ceph采用了高适用能力的分布式元数据集群架构，这极大地提高了元数据访问的可伸缩性，并因此提高了整个系统的可伸缩性。我们将讨论那些驱动我们选择了这种架构的目标与负载假设、分析它们对系统可伸缩性和性能的影响、并与我们在实现功能性系统原型的经验相结合。

## 2. 系统总览

Ceph文件系统有3个主要部件：（1）客户端：为主机或进程暴露类POSIX文件系统接口的实例；（2）OSD集群：共同存储所有的数据与元数据；（3）元数据服务器集群：管理命名空间（文件名与目录），同时协调安全性（security）、一致性（consistency）和连续性（coherence）。如**图1**所示。我们称Ceph接口是类POSIX接口，因为我们为了更好地与应用程序的需求对齐并改进系统性能，我们对POSIX接口进行了适当的扩展，并选择性地放松了一致性语义。

![图1 系统架构。客户端通过直接与OSD通信来执行文件I/O。每个进程既可以直接链接到客户端实例，也可以通过一个挂载的文件系统交互。](figure-1.png "图1 系统架构。客户端通过直接与OSD通信来执行文件I/O。每个进程既可以直接链接到客户端实例，也可以通过一个挂载的文件系统交互。")

该架构的主要目标是伸缩性（用于数百PB或更多）、性能、和可靠性。伸缩性被从多维度考虑，包括系统整体的存储容量、吞吐量、和单个客户端、目录或文件的性能。我们的目标负载可能包括一些极端情况，如数万或数十万个主机并发地读取或写入同一个文件，或在同目录下创建文件。这种场景在运行在超级计算机集群上的科学计算程序中很常见，并在未来的通用的负载中越来越多地起着指示作用。更重要的是，我们发现分布式文件系统的负载本质上是动态的，随着活动的应用程序和数据集随着时间的变化，数据与元数据的访问也显著的变化。Ceph直接解决了伸缩性的问题，同时通过三个基本设计特性解决了高性能、可靠性和可用性，这三种设计为：数据与元数据解耦、动态分布式元数据管理、和可靠的自主分布式对象存储。

**数据与元数据解耦：** Ceph将文件元数据与管理与文件数据的存储最大化地进行了分离。元数据操作（如open、rename等）被元数据集群共同管理，同时，客户端直接与OSD交互来执行文件I/O（读取和写入）。基于对象的存储通过将低级的块分配决策交给单个设备以提高文件系统的可伸缩性。然而，与现有的基于对象的文件系统<sup>[4, 7, 8, 32]</sup>不同，Ceph使用更短的对象列表替代较长的每个文件的块列表，因此完全消除了分配列表。文件数据被分条（strip）为可预测的命名对象上，同时通过一个叫CRUSH<sup>[29]</sup>的专用数据分布函数将对象分配到存储设备上。这让任一方都可以计算（而不是查找）组成文件内容的对象的名称与位置，消除了维护与分发对象列表的需求，简化了系统设计，减少了元数据集群的负载。

**动态分布式元数据管理：** 因为文件系统元数据操作组成了传统文件系统负载的一半<sup>[22]</sup>，高效的元数据管理对整个系统的性能非常重要。Ceph使用了一种基于动态子树分区（Dynamic Subtree Partitioning）<sup>[30]</sup>的新式元数据集群架构，并智能地将管理文件系统目录层级的责任分布到数十甚至数百个MDS上。（动态的）分层分区在每个MDS的负载都保持了局部性，这促进了高效地更新和主动式预取来改进常见的负载的性能。重要的是，元数据服务器间的负载分布完全基于当前的访问模式，使Ceph在任何负载下都能够高效利用可用的DMS资源，并实现与MDS数量呈近线性的伸缩性。

**可靠的自主分布式对象存储：** 由成千上万个设备组成了大型系统本质上是动态的：它们是被增量构建的，当部署新存储或退役旧设备时，它们会跟着伸展或收缩，且大量的数据会被创建、移动、和删除。所有的这些因素都要求数据的分布变得能够高效利用可用资源并维护所需的数据副本等级。Ceph把数据迁移、复制、故障检测、和故障恢复的责任托付给了存储数据的OSD集群，同时在上层，OSD共同为客户端和元数据服务器提供一个逻辑对象存储。这种方法让Ceph能够更高效地利用每个OSD的智能（CPU和内存）来实现可靠的、高可用的、有着线性伸缩性的对象存储。

本文中，我们描述了Ceph的客户端的操作、元数据服务器集群、和分布式对象存储，还描述了我们的架构怎样影响它们的关键特性。我们还描述了我们原型的状态。

## 3. 客户端操作

我们将介绍Ceph组件的整体操作，并通过描述Ceph的客户端的操作的方式介绍它们与应用程序的交互。Ceph的客户端运行在每个执行应用程序代码的主机上，并为应用程序暴露文件系统接口。在Ceph原型中，完全运行在用户空间，且既可以直接链接到它来访问，又可以通过FUSE（一种用户空间文件系统接口）<sup>[25]</sup>作为挂载的文件系统访问。每个客户端维护它拥有的文件数据缓存，该缓存与内核也或缓冲区缓存独立，使直接链接了客户端的应用程序能够访问它。

### 3.1 文件I/O和功能

当进程打开一个文件时，客户端会向MDS集群发送一个请求。MSD会遍历文件系统的层次结构，将文件名转换为文件的inode，inode中包括唯一的inode编号、文件所有者、模式、大小、和其他的单文件元数据。如果文件存在并有访问权限，MDS会返回inode编号、文件大小和将文件数据映射到对象的分条策略的信息。MDS可能还会向客户端发送（如果其还没收到过）指定了哪些操作是被允许的功能（capability）信息。目前，这些功能包括4位，其控制客户端的读（read）、缓存读（cache reads）、写（write）、和缓冲写（buffer writes）的能力。在未来，其功能还将包括允许客户端向OSD证明其被授权了读写数据权限的安全密钥<sup>[13, 19]</sup>（目前，原型信任所有客户端）。后续操作中，MDS在文件I/O中的参与仅限于管理功能，以维护文件的一致性并实现适当的语义。

Ceph有很多将文件数据映射到一系列对象上的分条策略。为了避免任何对与新文件分配（allocation）相关元数据的需求，对象名简单地将文件inode编号和条带（stripe）号。接着，会使用CRUSH将对象的副本分配到OSD上，CRUSH是全局可知的映射函数（在[章节5.1](#51-)中介绍）。例如，如果一个或多个客户端打开了一个文件用来读取，MDS会为它们授权读取和缓存文件内容的功能。有了inode编号、布局、和文件大小，客户端可以命名并定位所有包含文件数据的对象，并能够直接从OSD集群读取。任何不存在的对象或字节区间被定义为文件的“洞”，或者“零值”。类似地，如果客户端打开了文件用来写入，它会被授权带缓存的写入的功能，其在该文件的任何偏移量上生成的任何数据会被简单地写入到适当OSD上的适当对象中。客户端在文件关闭时会放弃（relinquish）对应的功能，并向MDS提供新文件的大小（写入的最大偏移量），这会重新定义（可能）已有的包含文件数据的一系列对象。

### 3.2 客户端同步

POSIX要求读取操作能够反映之前写入的任何数据，且写入操作时原子性的（也就是说，重叠的并发写入将会反映一个特定的写入顺序）。当文件被多个客户端打开时（多个writer或既有writer又有reader），MDS将会收回之前任何的缓存读取和缓冲区写入功能，强制同步客户端对该文件I/O。这样，每个应用程序的读取或写入操作将会被阻塞，直到OSD确认，这增加了OSD中存储的每个对象的串行更新和同步的负担。当写入跨对象边界时，客户端会请求对受影响的对象的独占锁（由这些对象所在的各自OSD授权），并立即提交写入并解锁操作，以实现所需的串行性。对象锁还可被用于在大型写入时，通过获取锁并异步冲刷（flush）数据来掩盖延迟。

意料中的是，同步I/O对应用程序的性能有很大的影响，特别是对那些进行小规模读写的应用程序来说更加明显，这时延迟造成的，其至少需要与OSD的一次往返的延迟。尽管在通用的负载中，读写共享的情况相对比较少<sup>[22]</sup>，但是在可写计算应用程序中，这种场景是非常常见的<sup>[22]</sup>，且这种情况下性能通常很重要。因此，当应用程序不需要依赖严格的标准一致性时，通常希望能够放松一致性来减少开销。尽管Ceph支持通过全局的开关来放松一致性，正如许多其他分布式文件系统在该问题上做的一样<sup>[20]</sup>，但是这是一种不精确且不能令人满意的觉接方案：要么性能会下降，要么会在系统范围下丢失一致性。

正由于这个原因，高性能计算（high-performance computing，HPC）社区<sup>[31]</sup>提出了一系列的对POSIX I/O的高性能计算扩展接口，Ceph中实现了这些接口中的一个子集。其中最引人注意的是，open操作的`O_LAZY`标识符允许应用程序显式地放松对共享写文件通常的连续性要求。管理自己连续性（例如HPC负载中常见的模式，通过写入同一个文件的不同部分）的性能敏感型程序在执行I/O时就可以通过缓冲区写入或通过缓存读取，否则只能同步执行。如果需要，应用程序可以进一步显式地通过两种额外的调用进行同步：`lazyio_propagate`会将给定的字节区间冲刷到对象存储中、`lazyio_synchronize`会确保过去的修改会在任何后续的读取中反映。因此，为了保持Ceph同步模型的简单性，其在客户端间通过同步I/O提供正确的*读-写*和*共享写*语义，并扩展了应用程序接口来放松性能敏感的分布式程序的一致性。

### 3.3 命名空间操作

客户端与文件系统命名空间的交互由元数据服务器集群管理。读操作（如readdir、stat）和更新（如unlink、chmod）都由MDS同步应用，以确保串行、一致性、正确的百密性（correct security）、和安全性（safety）。为了简单起见，客户端不使用元数据锁或租约。特别是对于HPC的负载，回调能够提供好处很小，但复杂性的潜在开销很高。

相反，Ceph为大多数通用元数据访问场景做了优化。在readdir之后对每个文件执行stat（例如，`ls -l`）是一个非常常见的访问模式，且是在大目录下臭名昭著的性能杀手。Ceph中的readdir仅需一次MDS请求，它会拉取整个目录。包括inode的内容。默认情况下，如果readdir后面会立刻接一个或多个stat，那么被缓存的简短的信息会被返回；否则，缓存的信息就被丢弃。虽然这种方法在中间inode修改可能不会被注意到的情况下稍稍放松了连续性，但是我们还是十分乐于通过这种交换来大幅改进性能。这种行为可被readdirplus<sup>[31]</sup>扩展显式地捕捉到，它会返回整个目录的lstat的结果（正如再一些专用OS的实现中getdir已经做的那样）。

Ceph可以通过更久地缓存元数据来允许一致性被进一步放松，这很像早期版本的NFS做的那样，其通常缓存30秒。然而，这种方法会在某种程度上打破连续性，通常这对应用程序来说是很重要的，比如那些使用stat来判断一个文件是否被更新过的应用程序。如果这样做，这些应用程序可能会执行不正确的行为，或要等待旧的缓存值超时。

我们再次选择了提供正确的行为并扩展了对性能有不利影响的接口。这种选择可通过下例清楚的说明：对一个被多个客户端并发为写入而打开的文件的stat操作。为了返回正确的文件大小和修改时间，MDS会收回任何写入的功能，以立刻停止更新并采集最新的大小和所有writer的mtime值。其中最大值会被返回，随后被撤销的功能会被重新下发以执行后续进程。尽管停止多个writer似乎有些过于激进，但为了保证合理的串行化，这时必需的。（对于单个writer，可以从正在写入的客户端检索到正确值，而不需要打断进程。）不需要连续性行为的应用程序（也就是需求与POSIX接口不一致的受害者）可以使用statlite<sup>[31]</sup>，其通过一个位掩码来执行哪些inode字段不需要连续性。

## 4. 动态分布式元数据

