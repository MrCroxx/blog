---
title: "《CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data》论文翻译[持续更新中]"
date: 2020-10-01T16:28:29+08:00
lastmod: 2020-10-01T16:28:33+08:00
draft: false
keywords: []
description: ""
tags: ["CRUSH", "Translation"]
categories: ["Paper Reading"]
author: ""
resources:
- name: featured-image
  src: paper-reading.jpg
---

*本篇文章是对论文[CRUSH: Controlled, Scalable, Decentralized Placement of Replicated Data](http://tom.nos-eastchina1.126.net/weil-crush-sc06.pdf)的原创翻译，转载请严格遵守[CC BY-NC-SA协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)。*


<!--more-->

## 摘要

新兴的大型分布式存储系统面临着将PB级的数据分布到数十、数百、甚至数千个存储设备上的问题。这样的系统必须均匀地分布数据和负载，以高效地利用可用资源并最大化系统性能，同时帮助处理增长并管理硬件故障。我们开发了CRUSH，一个可伸缩的伪随机数据分布函数，其为分布式对象存储系统设计，能高效地将数据对象映射到存储设备上，且而不依赖中央目录。因为大型系统有着天然的（inherently）动态性（译注：在Ceph中我将其翻译为“xxx本质上具有动态性”，本文中均翻译为“xxx具有天然的动态性”，以便于表达），CRUSh是为在帮助处理存储的增加与移除的同时减小不必要的数据移动而设计的。该算法适用于很多种类的数据副本和可靠性机制，同时根据用户定义的策略分布数据，这些策略可以强制将不同的副本分散到不同的故障域（failure domain）中。

## 1. 引言

对象存储是一种新兴的架构，它可以改善可管理性、可伸缩性和性能[Azagury et al. 2003]。与传统的基于块的硬盘驱动器不同，对象存储设备（object-based storage device，OSD）在内部管理磁盘块的分配，暴露可以让其它设备读取或写入到边长、命名对象的接口。在这种系统中，每个文件的数据通常会分条（strip）到相对小数量的命名对象中，而命名对象会分布在整个集群中。对象在多个设备间有多个副本（或者采用某些其他的数据冗余策略），以防止故障发生时数据丢失。基于对象的存储系统通过用娇小的对象列表替代较大的块列表，以简化数据布局并分摊低层级的块分配问题。虽然这通过减少了文件分配的元数据和复杂性大大提高了可伸缩性，但是将数据分布到数千个存储设备上（通常其容量和性能都不同）这一基本问题仍然存在。

大部分系统简单地将新数据写入到未被充分利用的设备上。这种方法的根本问题是，当数据被写入后，它很少甚至不会被移动。即使是完美的分布也会在存储系统被扩展后变得不均衡，因为新的磁盘或者是空的或者仅含有新的数据。旧的磁盘和新的磁盘可能都是繁忙的，这取决于系统负载，但是能够平等地利用二者并得到所有可用资源的优势的情况是非常罕见的。

一种健壮的解决方案是将所有数据随机分布到系统中可用的存储设备上。这样可以使分布在概率上是均衡的，且新数据和旧数据会均匀地混合在一起。当添加新的存储时，已有数据的中的一份随机采样会被迁移到新的存储设备上以保持平衡。这种方法最重要的优势是平均，所有设备将会有相似的负载，让系统在任何潜在负载下表现良好[Santos et al. 2000]。另外，在大型存储系统中，一个大文件会被随机的分布到很多个可用的设备上，这可以提供很高的并行性和整体带宽。然而，简单的基于哈希的分布方法无法很好应对设备数量变化的情况，它会导致数据大量重新调配（reshuffle）。此外，已有的随机分布策略会将每个磁盘上的副本分散到许多其它设备上，它在多个设备同时故障时由很大的数据丢失的可能性。

我们开发了CRUSH（Controlled Replication Under Scalable Hashing，基于可伸缩哈希的受控副本化方法），它是一个伪随机数据分布算法，它能够高效、健壮地将对象副本在异构、结构化存储集群上分布。CRUSH的实现是一个伪随机、确定性的函数，它将输入值（通常是一个对象或对象组的标识符）映射到一系列存储对象副本的设备上。它与传统方法的不同点在于，它的数据分配不依赖每个文件或每个对象的任何类型的目录——CRUSH仅需要对构成存储集群的设备的紧凑的层级描述和副本分配策略的知识。这种方法有两个关键的好处：第一，它完全是分布式的，大型系统中的任意一方都能单独计算任何对象的位置；第二，所需的元数据很少且几乎是静态的，它仅在有设备加入或移除时变化。

CRUSH是为优化数据分布以利用可用资源、在增加或移除存储设备时高效重新组织数据、和为数据副本分配提供灵活的约束以在意外或相关硬件故障时增加数据安全性设计的。其支持很多种数据安全机制，包括n路副本（镜像）、RAID奇偶校验策略或其它纠删码格式、和混合方法（例如RAID-10）。这些特性让CRUSH可以很好地适用于在可伸缩性、性能、和可靠性很重要的大型存储系统中（数PB）管理对象分布。

## 2. 相关工作

对象存储最近作为一种提高存储系统可伸缩性的机制备受关注。许多研究和生产级文件系统都采用了基于对象的方法，这包括影响深远的NASD文件系统[Gobioff et al. 1997]、Panasas文件系统[Nagle et al. 2004]、Lustre[Braam 2004]、和其它等基于对象的文件系统[Rodeh and Teperman 2003; Ghemawat et al. 2003]。其它的基于块的分布式文件系统像GPFS[Schmuck and Haskin 2002]和Federated Array of Bricks（FAB）[Saito et al. 2004]面临着类似的数据分布问题。这些系统使用了半随机（semi-random）或基于启发式（heuristic-based）的方法来将新数据分配到有可用容量的存储设备上，但是很少将数据重新放置以随着时间维护均衡的分布。更重要的是，所有的这些系统都通过某种元数据目录的方式定位数据，相反，CRUSH依赖紧凑集群描述（compact cluster description）和确定性映射函数（deterministic mapping function）。二者在写入数据时的区别最明显，使用CRUSH的系统不需要咨询中央分配器（central allocator）就能计算出任何新数据的存储位置。Sorrento[Tang et al. 2004]存储系统使用的一致性哈希[Karger et al. 1997] 与CRUSH最相似，但是其不支持设备权重控制（controlled weighting of devices）、数据良好均匀分布（well-balanced distribution of data）、和用来提供安全性的故障域（failure domains）。

尽管数据迁移问题已经在有显式分配映射的系统中有大量的研究[Anderson et al. 2001; Anderson et al. 2002]，这种方法对元数据的要求很高，而像CRUSH等方法想避免这一点。Choy等人[1996]描述了再次攀上分布数据的算法，其在磁盘增加时可以移动一定数量的对象以获得优化，但是不支持权重、副本、或磁盘移除。Brinkmann等人[2000]使用了哈希函数来将数据分布到异构但静态的集群中。SCADDAR[Goel et al. 2002]解决了添加或移除存储的问题，但是仅支持副本策略的一个受限的子集。这些方法都没有CRUSH的灵活性或用来提高可靠性的故障域的概念。

CRUSH基于RUSH算法族[Honicky and Miller 2004]且与其最为相似。RUSH是已有的文献中唯一的一个利用映射函数替代显式元数据并支持高效的加权设备添加或移除的算法。尽管RUSH算法有这些特性，但是大量的问题使其无法实际应用。CRUSH完整地包括了RUSH<sub>P</sub>和RUSH<sub>T</sub>中好用的元素，同时解决了之前未解决的可靠性和副本问题，并优化了性能和灵活性。

## 3. CRUSH算法

CRUSH算法参考每个设备的权重值来将数据对象从概率上近似均匀地分布到存储设备上。该分布受分层（hierarchical）*集群映射（cluster map）*控制，其表示可用的存储资源，且由这些资源组成的逻辑元素构成。例如，人们可能通过如下方式描述一个大型设施（installation）：设施由多行服务器的cabinet（机箱架）组成，cabinet中装满了disk of shelf（磁盘柜，译注：下文中省略为shelf），shelf中装满了storage device（存储设备，译注，下文中省略为device）。数据分布策略是根据*放置规则（placement rules）*定义的，这些规则指定了有多少目标副本被从集群中选取且副本放置时有哪些限制。例如，用户可能指定三份副本需要被放置在不同cabinet上的物理机上，这样它们就不会共享同一个电路。

给定一个整型输入值$x$,CRUSH会输出一个由$n$个不同的目标存储组成的有序列表$\overrightarrow{R}$。CRUSH使用了一个强大的多输入整型哈希函数，$x$是其输入之一，该函数可以使映射完全是确定性的，且可以仅通过集群映射、放置规则、和$x$计算。该分布是伪随机的，因为相似数据的输出结果或存储在任何device上的项（item）间没有明显的关联。CRUSH生成的副本分布是*非聚簇（declustered）*的，因为一个项（item）的副本所在的device与所有其它项的看上去也是独立的。

### 3.1 分层集群映射

集群映射由*device*和*bucket（桶）*组成，二者都有相关的数字型标识符和权重。bucket可以包括任意数量的device或其它的bucket，因此bucket可以在存储层次中作为中间节点，而device永远都是叶子节点。管理员为device分配权重，以控制它们负责存储的数据的相对的量。尽管大型系统很可能有不同容量、不同性能特征的device，但在随机数据分布部中，从统计学的角度看device利用率和负载相关，因此device的负载与存储的数据量成正比。因此，一维的放置矩阵（即权重）应从device容量得出。bucket的权重被定义为它包含的项的权重的和。

bucket可被随意组合来构建表示可用存储的层次结构。例如，用户可以创建如下的集群映射：在最下层使用“shelf（机柜）”来表示被安装的相同的device的集合，然后将多个“shelf”合并为“cabinet（cabinet）”bucket来将安装在同一个rack（机架）上的组合在一起。在更大的系统中，“cabinet”可能被进一步分组为“row（行）”或“room（房间）”bucket。数据通过一个伪随机类哈希函数在层次结构中递归选取嵌套的bucket项来放置。在传统的哈希技术中，对目标容器（device）数量的任何变更都会导致大量的容器内容重新调配；相反，CRUSH基于4中不同的bucket类型，每种类型都有不同的选取算法来解决device增加或移除带来的数据移动问题和整体的计算复杂度。

### 3.2 副本放置

CRUSH旨在将数据均匀地分布到加权的device上，以维护存储和device带宽资源利用的统计上的均衡。副本在层级结构的device上的放置同样对数据安全性有重要影响。CRUSH可以反映出设施的物理组织方式，并通过它对潜在的“关联（correlated）设备故障源”建模，并解决这些问题。典型的关联设备源包括共享的电源、和共享的网络。通过将这一信息编码到集群映射中，CRUSH放置策略可以在维护所需的分布同时将对象的副本分散到不同的故障域中。例如，为了解决可能发生的关联故障，可能需要确保数据副本分布在不同的cabinet、rack、电源、控制器、和/或物理位置上。

为了适配各种可能使用CRUSH的场景，无论是在数据副本策略还是在底层硬件配置方面，CRUSH为每个才用的副本策或分部策略都定义了放置规则，这让存储系统或管理员可以精确地指定对象副本如何放置。例如，一条选取了2个目标的规则采用2路镜像、一条选取了3个在不同数据中心的的目标的规则采用3路镜像、一条规则在5个device上采用RAID-4，等等<sup>注1</sup>。

> 注1：尽管各种各样的数据冗余机制都是可行的，为了简单起见，我们提到的数据对象都被采用副本策略存储。

![算法1 CRUSH中对象$x$的放置](algorithm-1.png "算法1 CRUSH中对象$x$的放置")

每条规则由一系列在简单的执行环境下被应用到层次结构中的操作组成，如**算法1**中的伪代码表示的那样。CRUSH函数的整型输入$x$，通常是对象名或其它标识符，如一组副本被放在同一个device上的一组对象的标识符。$take(a)$操作会在存储层次结构中选取一个项（通常是一个bucket），并将其分配给向量$\overrightarrow{i}$，作为后续操作的输入。$select(n,t)$操作遍历每个元素$i$（$i \in \overrightarrow{i}$），并在以该点为根的子树中选取$n$个类型为$t$的不同的项。device的类型已知且固定，且系统中的每个bucket都有一个类型字段，以用来区分不同类型的bucket（例如，用来表示“机柜行”的bucket和用来表示“cabinet”的bucket）。对于每个$i$（$i \in \overrightarrow{i}$），$select(n,t)$调用会遍历项$r$（$ r \in 1,...,n $），并递归下降地处理任何的中间bucket，然后在每个bucket中通过函数$c(r,x)$（在[章节3.4](#34-)中为每类bucket定义的）伪随机地选取一个其中的项，直到它找到一个请求的类型为$t$的项。其产生的$n| \overrightarrow{i} |$个不同的项目被放回到输入$ \overrightarrow{i} $中，或组成后续$select(n,t)$，或通过$emit$操作被移动到结果向量中。

![表1 在同一列但不同的3个cabinet上分布3份副本的简单规则。](table-1.png "表1 在同一列但不同的3个cabinet上分布3份副本的简单规则。")

例如，**表1**中定义的规则从**图1**中的层次结构的根开始。第一个$select(1,row)$选取了一个类型为“row”的bucket（其选取了row2）。随后的$select(3,cabinet)$在之前选取的row2内选择了3个不同的cabinet（cab21、cab23、cab24），最后，每个$select(1,disk)$遍历其输入向量中的3个cabinet bucket之一，并选择其下的1个disk。最终结果是分散在3个cabinet中的3个disk，但是都在同一个row中。因此，这种方法让副本能跨副本分布且对容器类型（例如，row、cabinet、shelf）进行约束，这一性质兼顾了可靠性和性能。规则由多个$take$、$emit$块组成，允许从不同的存储池中显式地提取目标存储，就像远程副本场景期望的那样（在该场景下，副本被存储在一个远程站点中）或分层设施场景期望的那样（例如，有快速近线存储和较慢但容量更大的存储阵列）。

![图1 一个由磁盘（disk）、磁盘柜（shelf of disk）、cabinet（cabinet）、行（row）组成的4层集群映射的局部视图。粗线条表示放置规则中每个$select$操作选取的项和**表1**中假设的映射。](figure-1.png "图1 一个由磁盘（disk）、磁盘柜（shelf of disk）、cabinet（cabinet）、行（row）组成的4层集群映射的局部视图。粗线条表示放置规则中每个$select$操作选取的项和表1中假设的映射。")

#### 3.2.1 碰撞、故障与过载

$select(n,t)$操作可能从它的起始点开始穿过多个存储层级以找到其下的$n$个不同的特定类型$t$的项，它是一个$r$（$r=1,...n$，n是所选的副本数）作为部分参数的递归进程。在此进程中，CRUSH可能因三个不同原因通过变更过的输入$r'$拒绝（reject）并重新选取（reselect）项：如果项已经被选取到当前的集合中（碰撞——$select(n,t)$的结果必须是不重复的），如果device是*故障的（failed）*，或者如果device是*过载的（overloaded）*。故障或过载的device会在集群映射中标记出来，但这些device还留在层级结构中，以避免不必要的数据迁移。CRUSH有选择性地迁移一个过载device上的数据，这通过按集群映射中指定的概率伪随机拒绝（reject）实现——这通常与device报告的过度利用的情况有关。对于故障或过载的device，CRUSH重启$select(n,t)$开始处的递归（见**算法1**第11行）均匀地将它们的项分布到存储集群中。如果发生碰撞，在递归的内层尝试本地搜索时首先会使用另外的$r'$（见**算法1**第14行）并避免整体数据分布偏离更可能发生冲突的子树（例如，bucket数小于n的子树）。

#### 3.2.2 副本排名

奇偶校验和纠删码策略与副本策略相比有明显不同的放置需求。在主拷贝副本策略（primary copy replication scheme）中，副本常常希望在之前的目标副本（已经有一份数据拷贝的副本）故障后成为新的主副本。在这种情况下，CRUSH可以用$ r ' = r + f $来重新选取“前n个”合适的目标，其中$f$是当前的$select(n,t)$的失败的放置尝试次数（见**算法1**第16行）。然而，在奇偶校验或纠删码策略下，CRUSH输出中的device的排名或位置十分重要，因为每个目标保存数据对象的不同的位。特别是，如果一个device故障，CRUSH的输出列表$\overrightarrow{R}$的适当位置需要被替换，这样，列表中的其他device会保持相同的排名（即，$\overrightarrow{R}$中的位置，见**图2**）。在这种情况下，CRUSH会使用$ r ' = r + f_{r} n $重新选取，其中$f_{r}$是$r$的失败尝试的次数，因此，这为每个副本的排名定义了一个候选序列，每个候选序列从概率上与其它device的故障无关。相反，RUSH没有对故障设备的特殊处理。像其它已有的哈希分布函数一样，它隐式地假设了使用“前n个”方法在结果中跳过故障设备，使其适用于副本策略。

### 3.3 映射变更和数据移动

