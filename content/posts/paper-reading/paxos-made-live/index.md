---
title: "《Paxos Made Live - An Engineering Perspective》论文翻译 [持续更新中]"
date: 2020-09-24T11:42:43+08:00
lastmod: 2020-09-24T11:42:43+08:00
draft: false
keywords: []
description: ""
tags: ["Paxos", "Translation"]
categories: ["Paper Reading"]
author: ""
resources:
- name: featured-image
  src: paper-reading.jpg
---

*本篇文章是对论文[Paxos Made Live - An Engineering Perspective](http://www8.cs.umu.se/kurser/5DV153/HT14/literature/chandra2006paxos.pdf)的原创翻译，转载请严格遵守[CC BY-NC-SA协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)。*


<!--more-->

## 摘要

我们描述了我们在使用Paxos共识算法构建一个容错的数据库的经历。尽管在该领域已经有文献，但是事实上构建这样一个数据库并非易事。我们描述了选取算法和遇到的工程问题，及我们对这些问题的解决方案。我们的度量表明我们构建了一个很有竞争力的系统。

## 1. 引言

众所周知，在商用硬件上的容错可通过副本实现<sup>[17, 18]</sup>。一个更通用的方法是使用共识算法<sup>[7]</sup>来确保所有副本相互一致<sup>[8, 14, 17]</sup>。通过对输入值的序列反复应用这样的算法，使为每个副本构建相同的值的日志成为可能。如果值是对某个数据结构的操作，那么建立在所有副本的相同日志上的应用程序可被用作实现所有副本中相互一致的数据结构。例如，如果日志由数据库操作序列组成，且对每个副本上的（本地）数据库应用了相同的操作序列，那么最终所有副本会有相同的数据库内容（前提是它们都从相同的初始数据库状态开始）。

这种通用的方法可被用作实现很多种容错基本组件，可容错数据库仅是一个例子。因此，在过去20年中，共识问题被大量研究。其中有几个众所周知的共识算法，它们可以在多种设置下执行，并能够容忍各种故障。Paxos算法<sup>[8]</sup>已经被从理论<sup>[16]</sup>和应用<sup>[10, 11, 12]</sup>的社区中被讨论了超过10年。

我们使用Paxos算法（Paxos）作为实现了容错日志的框架的基础。接着，我们依赖这个框架构建了一个容错数据库。尽管在这个方向已经有文献，但是构建一个生产级的系统并非易事，其原因有如下几点：

- 尽管Paxos能被一页伪代码描述出来，但是我们的完整实现却有几千行C++代码。代码量的爆炸并非由于我们使用了C++而不是伪代码，也不是由于我们的代码风格很啰嗦。将算法转化为实用的、可用于生产的系统，需要实现很多特性和优化——其中一些在文献中发表过，而一些却没有。

- 容错算法社区习惯于证明短算法的正确性（一页伪代码）。但是这种方法不适用于证明由几千行代码组成的系统的正确性。为了在真实系统中获取对“正确性”的自信，我们使用了很多不同的方法。

- 容错算法能够容忍一系列被小心选取的故障。然而，真实世界中的软件面对着各种各样的故障模式，包括算法中的错误、实现中的bug、和操作错误。我们必须设计软件并设计操作程序，以更健壮地处理更大范围的故障模式。

- 一个真是的系统很少能够精确地定义。甚至更糟的是，定义可能在实现阶段变化。因此，系统的实现应该是易修改的。最后，系统构建可能因为在其定义阶段的误解而失败。

本文挑选了我们在将Paxos从理论搬到实践过程中遇到的一些算法和工程挑战。这一工作比直接将伪代码翻译为C++多出很多研究与开发上的努力。

本文剩下的部分按照如下方式组织。接下来两章将展开讲解本项目的动机并描述我们构建的系统所处的一般环境。接着，我们重新回顾一下Paxos。我们将我们的经验分为三类并一次讨论：文献中算法的漏洞、软件工程上的挑战、和没有预期到的故障。最后我们通过度量我们的系统来进行总结，并对我们的领域的技术状况进行了更广泛的观察。

## 2. 背景

Chubby<sup>[1]</sup>是Google的容错系统，其提供了分布式锁机制并存储小文件。通常，每个数据中心有一个Chubby实例，或成为“cell”。一些Google的系统，如Google File System（GFS）<sup>[4]</sup>和Bigtable<sup>[2]</sup>，使用Chubby进行分布式协作并存储少量元数据。

Chubby通过副本的方式来实现容错。通常，一个Chubby cell有5个运行相同代码的副本组成，每个副本都运行在一个专用的机器上。每个Chubby对象（例如Chubby锁、或Chubby文件）被作为数据库的一个条目存储。因此其实这些就是数据库的副本。在任意时间，这些副本之一会作为“master”。

Chubby的客户端（例如GFS和Bigtable）通过与Chubby cell通信来获取服务。master副本为所有Chubby请求提供服务。如果Chubby通信的副本不是master，那么该副本会回复master副本的网络地址。随后Chubby可以联系master副本。如果master宕机，那么新的master会被自动选举，随后新的master副本会基于其本地的数据库副本继续提供服务。因此，数据库的副本保证了在master故障转移过程中Chubby状态的连续性。

Chubby的第一个版本基于第三方商业容错数据库，我们在本文的余下部分称这个数据库为“3DB”。这个数据库有与副本相关的bug历史。事实上，据我们所知，其副本机制没有基于被证明过的算法，且我们不知道其是否正确。考虑到这个产品的历史原因和Chubby的重要性，我们最终决定使用我们自己的基于Paxos算法的方案替换3DB。

## 3. 架构概要

**图1**阐述了单Chubby副本的架构。基于Paxos算法的容错副本日志坐落在协议栈的底层。每个副本维护一个本地的日志拷贝。Paxos算法按照需求反复运行以确保所有副本在它们的本地日志中要相同的条目序列。副本间互相通过Paxos专用的协议通信。

![图1 单Chubby副本](figure-1.png "图1 单Chubby副本")

下一层是一个多副本容错数据库，其每个副本都有一个数据库的本地拷贝。数据库由一个本地的snapshot（快照）和replay（重放日志，数据库操作的日志）组成。新的数据库操作会被提交到多副本的日志中。当数据库操作出现在副本中时，其会被应用到副本的本地数据库拷贝中。

最后，Chubby使用了容错数据库来存储其状态。Chubby的客户端通过Chubby专用协议与单个Chubby副本通信。

我们致力于设计将Paxos框架、数据库、和Chubby分离的清晰的接口。这部分是为了系统开发的清晰性，也同样为了能在其它应用程序中复用多副本日志层。我们预计Google之后的系统会通过副本的方式实现容错。我们相信容错日志对于构建这样的系统来说是一个强大的组件。

![图2 容错日志的API](figure-2.png "图2 容错日志的API")

我们的容错日志的API如**图2**所示。其包括一个用来将新的值提交（submit）到日志中的调用。一旦被提交的值进入容错日志，我们的系统会调用每个客户端应用程序中的回调，并传递被提交的值。

我们的系统是多线程的，多个值可在不同的线程中被并发提交。多副本的日志不会创建其自己的线程，但是可在任意数量的线程中被并发调用。这种线程化的系统有助于我们测试系统，我们将在后文中详细介绍。

## 4. 回顾Paxos

本章中我们将介绍基本Paxos算法的概要，并概括地介绍我们如何将运行的多个Paxos联系到一起（Multi-Paxos）。想要了解更多形式化的描述和正确性证明的读者可以参考文献<sup>[8, 9, 16]</sup>。熟悉Paxos的读者可以跳过这一章。

### 4.1 Paxos基础

Paxos是一种共识算法，由一个进程（被称为副本，replica）集合执行，用于在有故障的情况下就单个值达成一致。副本可能崩溃也可能随后恢复。网络可能丢失信息也可能导致信息重复。副本可以访问持久化存储，其可在崩溃时幸存。一些副本可以提交（submit）值以达成共识。如果最终大部分副本运行了足够长的时间而没有崩溃且没有故障，那么所有运行中的副本会对被提交的值之一达成一致（agree）。在我们的系统中，要达成一致的值是（多副本）日志的下一个条目，正如引言中描述的那样。

该算法由3个阶段组成，每个阶段都有可能重复（因为失败）：

1. 选举一个副本作为coordinator（协调者）。

2. coordinator选取一个值，并通过被称为“accept消息”的消息将其广播给所有的副本。其他副本或者“acknowledge（确认）”该消息，或者“reject（拒绝）”该消息。

3. 当大部分副本确认了该coordinator后，共识就会被达成，coordinator会广播一条“commit消息”来通知所有副本。

为了直观地了解该算法是如何工作的，首先考虑仅有一个coordinator且没有故障的情况。一旦大部分的副本收到了来自coordinator的accept消息并确认它后，就会达成共识。接下来，如果任意的少部分副本故障，我们仍能够保证至少一个收到了共识值的副本存活。

在现实中，coordinator可能故障。Paxos不需要在同一时间只有一个副本作为coordinator。在任何时间，都可以有多个副本可能决定变为coordinator并执行算法。通常，系统会被设计为限制coordinator的更替，因为其会推迟共识的达成。

这种宽松的选取策略意味着同时可能有多个认为自己是coordinator的副本。而且，这些coordinator可能选取了不同的值。Paxos通过两种额外的机制确保仅有一个值会达成共识（该值可能来自任一coordinator），这两个机制为：（1）对连续的coordinator排序（2）限制每个coordinator的选择中只能选取一个值。

对coordinator排序让每个副本都能区分当前的coordinator和过去的coordinator。通过这种方式，副本可以拒绝来自旧coordinator的消息，并防止这些消息破坏已达成的共识。Paxos通过给coordinator分配递增的序列号的方式对它们进行排序。每个副本追踪其上一次见到的序列号。当副本想要变为coordinator时，它会生成一个唯一的<sup>注1</sup>比其之前见过的更高的序列号，并将其在proposer消息中广播给所有副本。如果大部分副本作出回复并标明它们没有见过更大的序列号，那么该副本就会作为coordinator。这些回复被称为promise消息，因为副本承诺从此以后拒绝来自旧的coordinator的消息。proposer/promise消息交换构成了上面列出的步骤1。

> 注1：例如，在有$n$个副本的系统中，为每个副本$r$分配一个$0$到$n-1$的唯一的id $i_r$。副本$r$选取比起见过的序列号$s$要大的最小序列号，且$s \mod n = i_r$。

一旦对一个值的共识达成，Paxos必须强制后面的coordinator选取与其相同的值，以确保持续的一致。为了确保这一点，来自副本的promise消息包含它们上一次听说的值（如果存在）和它们听说的值来自的coordinator的序列号。新的coordinator选取最近的coordinator的值。如果promise消息都没包含值，那么coordinator可以自由地选取提交的值。

算法能工作的原理有些微妙，但大致如下。新的coordinator需要来自大多数副本的对proposer消息的响应。因此，如果之前的coordinator达成了一个共识，那么可以保证新的coordinator能够至少从一个副本听到决定的值。通过归纳，该值将会有所有收到的响应中最大的序列号，所以其将会被选取为新的coordinator。

### 4.2 Multi-Paxos

使用的系统使用Paxos作为构建单元来实现*值序列*的共识，如多副本日志。实现它的简单方式是反复执行Paxos算法。我们把每次执行称为Paxos算法的一个*实例（instance）*。“像Paxos提交（submit）一个值”表示“执行一个Paxos的实例同时提交该值”。

在Multi-Paxos中，一些缓慢（slow，lagging）的副本可能不会参与最近的Paxos实例。我们使用了*追赶（catch-up）*机制来使缓慢的副本能够追赶上leader副本。

每个副本维护了一个本地的持久化日志来记录Paxos的行为。当副本崩溃并随后恢复后，它会重放持久化日志来重构其崩溃前的状态。副本还会在帮助落后的副本追赶时使用这个日志。目前为止，我们描述的Paxos算法要求所有消息的发送者在发送消息前记录它们的状态——因此，该算法的关键路径上会对磁盘进行5次写入（每次proposer、promise、accept、acknowledgement、commit消息会写入一次）。需要注意的是，所有的写入在系统可以继续进行任何操作之前必须立即刷盘。在副本在网络中邻近的系统中，刷盘时间可能会主导该实现的整体延迟。

一个用来减少消息数的常用优化是将多个Paxos实例连接到一起<sup>[9]</sup>。如果coordinator身份不会在实例间发生变化，那么propsoer消息可以被省略。因为任何副本在任何时间仍然可以通过广播有更高序列号的proposer消息来试图变为coordinator，所以Paxos的性质不会受影响。为了利用这一优化，Multi-Paxos算法会被设计为选取一个coordinator并长时间保持，尽量不要使coordinator改变。我们称这样的coordinator为master。通过这种优化，